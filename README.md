# llm-gemma-app-
Une application simple et locale pour discuter avec un mod√®le de langage Gemma via Ollama, construite avec Streamlit.
# llm-gemma-app-
Une application simple et locale pour discuter avec un mod√®le de langage Gemma via Ollama, construite avec Streamlit.
üí¨ Gemma Chat App

# üí¨ Gemma Chat App  

[![Python](https://img.shields.io/badge/Python-3.11-blue)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.50.0-ff4b4b)](https://streamlit.io/)
[![Ollama](https://img.shields.io/badge/Ollama-Model%20Runner-black)](https://ollama.ai/)
[![License](https://img.shields.io/badge/License-Apache%202.0-green)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Active-success)](#)

Une application **Streamlit** qui te permet de discuter avec un mod√®le de langage **Gemma** local via **Ollama** üß†  
Aucune cl√© API, aucun co√ªt, 100 % local et priv√©.  

---

## üöÄ Fonctionnalit√©s
‚úÖ Interface conversationnelle moderne (type ChatGPT)  
‚úÖ Ex√©cution **locale** sans d√©pendance cloud  
‚úÖ Compatible avec tous les mod√®les Ollama (`gemma`, `llama3`, `mistral`, etc.)  
‚úÖ Code Python clair, facilement modifiable  

---

## üß© Pr√©requis

### 1Ô∏è‚É£ Installer Ollama  
T√©l√©charge et installe Ollama ici üëâ [https://ollama.ai/download](https://ollama.ai/download)

Puis t√©l√©charge le mod√®le **Gemma** :
```bash
ollama pull gemma


2Ô∏è‚É£ Installer les d√©pendances Python

Cr√©e un environnement virtuel (optionnel mais recommand√©) :
python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate sous Windows




Puis installe les d√©pendances :

pip install -r requirements.txt